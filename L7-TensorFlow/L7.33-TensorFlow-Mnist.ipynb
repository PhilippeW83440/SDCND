{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow mini batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll go over what mini-batching is and how to apply it in TensorFlow.\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(55000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data.\n",
    "\n",
    "You may have to look up how much memory a float32 requires, using this link: https://en.wikipedia.org/wiki/Single-precision_floating-point_format\n",
    "\n",
    "train_features Shape: (55000, 784) Type: float32\n",
    "\n",
    "train_labels Shape: (55000, 10) Type: float32\n",
    "\n",
    "weights Shape: (784, 10) Type: float32\n",
    "\n",
    "bias Shape: (10,) Type: float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172480000\n",
      "2200000\n",
      "31360\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(55000*784*4)\n",
    "print(55000*10*4)\n",
    "print(784*10*4)\n",
    "print(10*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory. You could train this whole dataset on most CPUs and GPUs.\n",
    "\n",
    "But larger datasets that you'll use in the future measured in gigabytes or more. It's possible to purchase more memory, but it's expensive. A Titan X GPU with 12 GB of memory costs over $1,000.\n",
    "\n",
    "Instead, in order to run large models on your machine, you'll learn how to use mini-batching.\n",
    "\n",
    "Let's look at how you implement mini-batching in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Mini-batching "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's **tf.placeholder()** function to receive the varying batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the example, if each sample had **n_input = 784** features and **n_classes = 10** possible labels, the dimensions for **features** would be **[None, n_input]** and **labels** would be **[None, n_classes]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does **None** do here?\n",
    "\n",
    "The **None** dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed **features** and **labels** into the model as either the batches of 128 samples or the single batch of 104 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the parameters below, how many batches are there, and what is the last batch size?\n",
    "\n",
    "features is (50000, 400)\n",
    "\n",
    "labels is (50000, 10)\n",
    "\n",
    "batch_size is 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "all = 50000\n",
    "batch_size = 128\n",
    "n_full_batch = 50000//128\n",
    "print(n_full_batch)\n",
    "last_batch_size = all-n_full_batch*batch_size\n",
    "print(last_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the **batches** function to batch **features** and **labels**. The function should return each batch with a maximum size of **batch_size**. To help you with the quiz, look at the following example output of a working **batches** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# example_batches = batches(3, example_features, example_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **example_batches** variable would be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['F11', 'F12', 'F13', 'F14'],\n",
       "   ['F21', 'F22', 'F23', 'F24'],\n",
       "   ['F31', 'F32', 'F33', 'F34']],\n",
       "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
       " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the **batches** function in the **quiz.py** file below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quiz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sandbox.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "#from quiz import batches\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the batches with the **batches** function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quiz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.10419999063014984\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    # for batch_features, batch_labels in ______\n",
    "    output_batches = batches(batch_size, train_features, train_labels)\n",
    "    for batch_features, batch_labels in output_batches:\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. You'll go over this subject in the next section where we talk about \"epochs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 8.66     Valid Accuracy: 0.289\n",
      "Epoch: 1    - Cost: 6.53     Valid Accuracy: 0.417\n",
      "Epoch: 2    - Cost: 5.12     Valid Accuracy: 0.504\n",
      "Epoch: 3    - Cost: 4.19     Valid Accuracy: 0.564\n",
      "Epoch: 4    - Cost: 3.57     Valid Accuracy: 0.61 \n",
      "Epoch: 5    - Cost: 3.13     Valid Accuracy: 0.646\n",
      "Epoch: 6    - Cost: 2.82     Valid Accuracy: 0.675\n",
      "Epoch: 7    - Cost: 2.57     Valid Accuracy: 0.696\n",
      "Epoch: 8    - Cost: 2.39     Valid Accuracy: 0.715\n",
      "Epoch: 9    - Cost: 2.23     Valid Accuracy: 0.73 \n",
      "Epoch: 10   - Cost: 2.11     Valid Accuracy: 0.743\n",
      "Epoch: 11   - Cost: 2.0      Valid Accuracy: 0.754\n",
      "Epoch: 12   - Cost: 1.91     Valid Accuracy: 0.764\n",
      "Epoch: 13   - Cost: 1.83     Valid Accuracy: 0.773\n",
      "Epoch: 14   - Cost: 1.76     Valid Accuracy: 0.779\n",
      "Epoch: 15   - Cost: 1.7      Valid Accuracy: 0.784\n",
      "Epoch: 16   - Cost: 1.65     Valid Accuracy: 0.788\n",
      "Epoch: 17   - Cost: 1.6      Valid Accuracy: 0.794\n",
      "Epoch: 18   - Cost: 1.55     Valid Accuracy: 0.798\n",
      "Epoch: 19   - Cost: 1.51     Valid Accuracy: 0.8  \n",
      "Epoch: 20   - Cost: 1.48     Valid Accuracy: 0.803\n",
      "Epoch: 21   - Cost: 1.44     Valid Accuracy: 0.805\n",
      "Epoch: 22   - Cost: 1.41     Valid Accuracy: 0.81 \n",
      "Epoch: 23   - Cost: 1.38     Valid Accuracy: 0.814\n",
      "Epoch: 24   - Cost: 1.36     Valid Accuracy: 0.817\n",
      "Epoch: 25   - Cost: 1.33     Valid Accuracy: 0.82 \n",
      "Epoch: 26   - Cost: 1.31     Valid Accuracy: 0.823\n",
      "Epoch: 27   - Cost: 1.29     Valid Accuracy: 0.825\n",
      "Epoch: 28   - Cost: 1.27     Valid Accuracy: 0.826\n",
      "Epoch: 29   - Cost: 1.25     Valid Accuracy: 0.827\n",
      "Epoch: 30   - Cost: 1.23     Valid Accuracy: 0.829\n",
      "Epoch: 31   - Cost: 1.21     Valid Accuracy: 0.83 \n",
      "Epoch: 32   - Cost: 1.2      Valid Accuracy: 0.833\n",
      "Epoch: 33   - Cost: 1.18     Valid Accuracy: 0.834\n",
      "Epoch: 34   - Cost: 1.17     Valid Accuracy: 0.834\n",
      "Epoch: 35   - Cost: 1.15     Valid Accuracy: 0.835\n",
      "Epoch: 36   - Cost: 1.14     Valid Accuracy: 0.837\n",
      "Epoch: 37   - Cost: 1.13     Valid Accuracy: 0.839\n",
      "Epoch: 38   - Cost: 1.12     Valid Accuracy: 0.84 \n",
      "Epoch: 39   - Cost: 1.11     Valid Accuracy: 0.842\n",
      "Epoch: 40   - Cost: 1.1      Valid Accuracy: 0.843\n",
      "Epoch: 41   - Cost: 1.09     Valid Accuracy: 0.844\n",
      "Epoch: 42   - Cost: 1.08     Valid Accuracy: 0.845\n",
      "Epoch: 43   - Cost: 1.07     Valid Accuracy: 0.846\n",
      "Epoch: 44   - Cost: 1.06     Valid Accuracy: 0.847\n",
      "Epoch: 45   - Cost: 1.05     Valid Accuracy: 0.847\n",
      "Epoch: 46   - Cost: 1.04     Valid Accuracy: 0.849\n",
      "Epoch: 47   - Cost: 1.04     Valid Accuracy: 0.849\n",
      "Epoch: 48   - Cost: 1.03     Valid Accuracy: 0.849\n",
      "Epoch: 49   - Cost: 1.02     Valid Accuracy: 0.849\n",
      "Epoch: 50   - Cost: 1.02     Valid Accuracy: 0.85 \n",
      "Epoch: 51   - Cost: 1.01     Valid Accuracy: 0.851\n",
      "Epoch: 52   - Cost: 1.0      Valid Accuracy: 0.852\n",
      "Epoch: 53   - Cost: 0.996    Valid Accuracy: 0.853\n",
      "Epoch: 54   - Cost: 0.99     Valid Accuracy: 0.854\n",
      "Epoch: 55   - Cost: 0.984    Valid Accuracy: 0.855\n",
      "Epoch: 56   - Cost: 0.979    Valid Accuracy: 0.855\n",
      "Epoch: 57   - Cost: 0.973    Valid Accuracy: 0.855\n",
      "Epoch: 58   - Cost: 0.968    Valid Accuracy: 0.856\n",
      "Epoch: 59   - Cost: 0.962    Valid Accuracy: 0.857\n",
      "Epoch: 60   - Cost: 0.957    Valid Accuracy: 0.858\n",
      "Epoch: 61   - Cost: 0.952    Valid Accuracy: 0.858\n",
      "Epoch: 62   - Cost: 0.947    Valid Accuracy: 0.858\n",
      "Epoch: 63   - Cost: 0.942    Valid Accuracy: 0.859\n",
      "Epoch: 64   - Cost: 0.937    Valid Accuracy: 0.859\n",
      "Epoch: 65   - Cost: 0.933    Valid Accuracy: 0.86 \n",
      "Epoch: 66   - Cost: 0.928    Valid Accuracy: 0.86 \n",
      "Epoch: 67   - Cost: 0.924    Valid Accuracy: 0.86 \n",
      "Epoch: 68   - Cost: 0.919    Valid Accuracy: 0.861\n",
      "Epoch: 69   - Cost: 0.915    Valid Accuracy: 0.861\n",
      "Epoch: 70   - Cost: 0.911    Valid Accuracy: 0.861\n",
      "Epoch: 71   - Cost: 0.907    Valid Accuracy: 0.862\n",
      "Epoch: 72   - Cost: 0.903    Valid Accuracy: 0.862\n",
      "Epoch: 73   - Cost: 0.898    Valid Accuracy: 0.863\n",
      "Epoch: 74   - Cost: 0.894    Valid Accuracy: 0.864\n",
      "Epoch: 75   - Cost: 0.891    Valid Accuracy: 0.865\n",
      "Epoch: 76   - Cost: 0.887    Valid Accuracy: 0.866\n",
      "Epoch: 77   - Cost: 0.883    Valid Accuracy: 0.867\n",
      "Epoch: 78   - Cost: 0.879    Valid Accuracy: 0.867\n",
      "Epoch: 79   - Cost: 0.875    Valid Accuracy: 0.868\n",
      "Epoch: 80   - Cost: 0.872    Valid Accuracy: 0.869\n",
      "Epoch: 81   - Cost: 0.868    Valid Accuracy: 0.869\n",
      "Epoch: 82   - Cost: 0.865    Valid Accuracy: 0.87 \n",
      "Epoch: 83   - Cost: 0.861    Valid Accuracy: 0.87 \n",
      "Epoch: 84   - Cost: 0.858    Valid Accuracy: 0.871\n",
      "Epoch: 85   - Cost: 0.854    Valid Accuracy: 0.871\n",
      "Epoch: 86   - Cost: 0.851    Valid Accuracy: 0.872\n",
      "Epoch: 87   - Cost: 0.847    Valid Accuracy: 0.872\n",
      "Epoch: 88   - Cost: 0.844    Valid Accuracy: 0.872\n",
      "Epoch: 89   - Cost: 0.841    Valid Accuracy: 0.872\n",
      "Epoch: 90   - Cost: 0.838    Valid Accuracy: 0.872\n",
      "Epoch: 91   - Cost: 0.834    Valid Accuracy: 0.873\n",
      "Epoch: 92   - Cost: 0.831    Valid Accuracy: 0.874\n",
      "Epoch: 93   - Cost: 0.828    Valid Accuracy: 0.873\n",
      "Epoch: 94   - Cost: 0.825    Valid Accuracy: 0.874\n",
      "Epoch: 95   - Cost: 0.822    Valid Accuracy: 0.874\n",
      "Epoch: 96   - Cost: 0.819    Valid Accuracy: 0.874\n",
      "Epoch: 97   - Cost: 0.816    Valid Accuracy: 0.875\n",
      "Epoch: 98   - Cost: 0.813    Valid Accuracy: 0.875\n",
      "Epoch: 99   - Cost: 0.81     Valid Accuracy: 0.875\n",
      "Epoch: 100  - Cost: 0.807    Valid Accuracy: 0.876\n",
      "Epoch: 101  - Cost: 0.804    Valid Accuracy: 0.877\n",
      "Epoch: 102  - Cost: 0.801    Valid Accuracy: 0.877\n",
      "Epoch: 103  - Cost: 0.798    Valid Accuracy: 0.878\n",
      "Epoch: 104  - Cost: 0.796    Valid Accuracy: 0.878\n",
      "Epoch: 105  - Cost: 0.793    Valid Accuracy: 0.878\n",
      "Epoch: 106  - Cost: 0.79     Valid Accuracy: 0.879\n",
      "Epoch: 107  - Cost: 0.787    Valid Accuracy: 0.879\n",
      "Epoch: 108  - Cost: 0.785    Valid Accuracy: 0.879\n",
      "Epoch: 109  - Cost: 0.782    Valid Accuracy: 0.879\n",
      "Epoch: 110  - Cost: 0.779    Valid Accuracy: 0.879\n",
      "Epoch: 111  - Cost: 0.777    Valid Accuracy: 0.879\n",
      "Epoch: 112  - Cost: 0.774    Valid Accuracy: 0.879\n",
      "Epoch: 113  - Cost: 0.771    Valid Accuracy: 0.88 \n",
      "Epoch: 114  - Cost: 0.769    Valid Accuracy: 0.88 \n",
      "Epoch: 115  - Cost: 0.766    Valid Accuracy: 0.881\n",
      "Epoch: 116  - Cost: 0.764    Valid Accuracy: 0.882\n",
      "Epoch: 117  - Cost: 0.761    Valid Accuracy: 0.882\n",
      "Epoch: 118  - Cost: 0.759    Valid Accuracy: 0.882\n",
      "Epoch: 119  - Cost: 0.756    Valid Accuracy: 0.883\n",
      "Epoch: 120  - Cost: 0.754    Valid Accuracy: 0.883\n",
      "Epoch: 121  - Cost: 0.751    Valid Accuracy: 0.883\n",
      "Epoch: 122  - Cost: 0.749    Valid Accuracy: 0.883\n",
      "Epoch: 123  - Cost: 0.746    Valid Accuracy: 0.883\n",
      "Epoch: 124  - Cost: 0.744    Valid Accuracy: 0.884\n",
      "Epoch: 125  - Cost: 0.742    Valid Accuracy: 0.884\n",
      "Epoch: 126  - Cost: 0.739    Valid Accuracy: 0.884\n",
      "Epoch: 127  - Cost: 0.737    Valid Accuracy: 0.884\n",
      "Epoch: 128  - Cost: 0.735    Valid Accuracy: 0.885\n",
      "Epoch: 129  - Cost: 0.732    Valid Accuracy: 0.885\n",
      "Epoch: 130  - Cost: 0.73     Valid Accuracy: 0.885\n",
      "Epoch: 131  - Cost: 0.728    Valid Accuracy: 0.885\n",
      "Epoch: 132  - Cost: 0.726    Valid Accuracy: 0.885\n",
      "Epoch: 133  - Cost: 0.723    Valid Accuracy: 0.885\n",
      "Epoch: 134  - Cost: 0.721    Valid Accuracy: 0.885\n",
      "Epoch: 135  - Cost: 0.719    Valid Accuracy: 0.885\n",
      "Epoch: 136  - Cost: 0.717    Valid Accuracy: 0.886\n",
      "Epoch: 137  - Cost: 0.715    Valid Accuracy: 0.886\n",
      "Epoch: 138  - Cost: 0.713    Valid Accuracy: 0.886\n",
      "Epoch: 139  - Cost: 0.71     Valid Accuracy: 0.886\n",
      "Epoch: 140  - Cost: 0.708    Valid Accuracy: 0.886\n",
      "Epoch: 141  - Cost: 0.706    Valid Accuracy: 0.886\n",
      "Epoch: 142  - Cost: 0.704    Valid Accuracy: 0.886\n",
      "Epoch: 143  - Cost: 0.702    Valid Accuracy: 0.886\n",
      "Epoch: 144  - Cost: 0.7      Valid Accuracy: 0.886\n",
      "Epoch: 145  - Cost: 0.698    Valid Accuracy: 0.886\n",
      "Epoch: 146  - Cost: 0.696    Valid Accuracy: 0.886\n",
      "Epoch: 147  - Cost: 0.694    Valid Accuracy: 0.887\n",
      "Epoch: 148  - Cost: 0.692    Valid Accuracy: 0.887\n",
      "Epoch: 149  - Cost: 0.69     Valid Accuracy: 0.887\n",
      "Test Accuracy: 0.8875001072883606\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "learn_rate = 0.01 #0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pythonx]",
   "language": "python",
   "name": "conda-env-pythonx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
